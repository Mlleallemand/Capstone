{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Starbucks-Capstone-Challenge\" data-toc-modified-id=\"Starbucks-Capstone-Challenge-1\">Starbucks Capstone Challenge</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.0.1\">Introduction</a></span></li><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-1.0.2\">Example</a></span></li><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-1.0.3\">Cleaning</a></span></li><li><span><a href=\"#Final-Advice\" data-toc-modified-id=\"Final-Advice-1.0.4\">Final Advice</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Sets\" data-toc-modified-id=\"Data-Sets-2\">Data Sets</a></span></li><li><span><a href=\"#OBJECTIVE\" data-toc-modified-id=\"OBJECTIVE-3\">OBJECTIVE</a></span></li><li><span><a href=\"#Data-exploration\" data-toc-modified-id=\"Data-exploration-4\">Data exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Portfolio-data-description\" data-toc-modified-id=\"Portfolio-data-description-4.1\"><strong>Portfolio data description</strong></a></span></li><li><span><a href=\"#Profile-data-description\" data-toc-modified-id=\"Profile-data-description-4.2\">Profile data description</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Check-for-missing-values\" data-toc-modified-id=\"Check-for-missing-values-4.2.0.1\">Check for missing values</a></span></li><li><span><a href=\"#Check-for-general-bias-in-profile-data\" data-toc-modified-id=\"Check-for-general-bias-in-profile-data-4.2.0.2\">Check for general bias in profile data</a></span></li><li><span><a href=\"#Assessing-differences-in-income-and-age-between-gender-groups\" data-toc-modified-id=\"Assessing-differences-in-income-and-age-between-gender-groups-4.2.0.3\">Assessing differences in income and age between gender groups</a></span></li><li><span><a href=\"#Assessing-differences-in-income-by-gender-(main-effect-of-gender)\" data-toc-modified-id=\"Assessing-differences-in-income-by-gender-(main-effect-of-gender)-4.2.0.4\">Assessing differences in income by gender (main effect of gender)</a></span></li><li><span><a href=\"#Assessing-differences-in-income-by-age-(main-effect-of-age)\" data-toc-modified-id=\"Assessing-differences-in-income-by-age-(main-effect-of-age)-4.2.0.5\">Assessing differences in income by age (main effect of age)</a></span></li><li><span><a href=\"#Assessing-differences-in-age-by-gender\" data-toc-modified-id=\"Assessing-differences-in-age-by-gender-4.2.0.6\">Assessing differences in age by gender</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-5\">Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Portfolio-data-preprocessing\" data-toc-modified-id=\"Portfolio-data-preprocessing-5.1\">Portfolio data preprocessing</a></span></li><li><span><a href=\"#Transcript-data-preprocessing\" data-toc-modified-id=\"Transcript-data-preprocessing-5.2\">Transcript data preprocessing</a></span></li><li><span><a href=\"#Profile-data-preprocessing\" data-toc-modified-id=\"Profile-data-preprocessing-5.3\">Profile data preprocessing</a></span></li></ul></li><li><span><a href=\"#Data-Analysis\" data-toc-modified-id=\"Data-Analysis-6\">Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exploratory-analysis:-offering-logic\" data-toc-modified-id=\"Exploratory-analysis:-offering-logic-6.1\">Exploratory analysis: offering logic</a></span></li><li><span><a href=\"#Is-there-a-difference-in-effectiveness-of-individual-offers,-independent-of-subject?\" data-toc-modified-id=\"Is-there-a-difference-in-effectiveness-of-individual-offers,-independent-of-subject?-6.2\">Is there a difference in effectiveness of individual offers, independent of subject?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-if-success,-miss,-ignore-and-control-rates-differ-significantly-for-offer-type,-difficulty-(high-vs.-low)-or-reward-(high-vs.-low)\" data-toc-modified-id=\"Check-if-success,-miss,-ignore-and-control-rates-differ-significantly-for-offer-type,-difficulty-(high-vs.-low)-or-reward-(high-vs.-low)-6.2.1\">Check if success, miss, ignore and control rates differ significantly for offer type, difficulty (high vs. low) or reward (high vs. low)</a></span></li><li><span><a href=\"#Check-if-the-sets-of-subjects-we-defined-are-mutually-exclusive\" data-toc-modified-id=\"Check-if-the-sets-of-subjects-we-defined-are-mutually-exclusive-6.2.2\">Check if the sets of subjects we defined are mutually exclusive</a></span></li><li><span><a href=\"#Can-we-improve-results-by-weighting-scores-with-attractiveness-of-the-offer?\" data-toc-modified-id=\"Can-we-improve-results-by-weighting-scores-with-attractiveness-of-the-offer?-6.2.3\">Can we improve results by weighting scores with attractiveness of the offer?</a></span></li></ul></li><li><span><a href=\"#Add-weighted-customer-responses-across-offer_ids\" data-toc-modified-id=\"Add-weighted-customer-responses-across-offer_ids-6.3\">Add weighted customer responses across offer_ids</a></span></li><li><span><a href=\"#Train-Random-Forest-Classifier-on-Success-vs.-Ignore\" data-toc-modified-id=\"Train-Random-Forest-Classifier-on-Success-vs.-Ignore-6.4\">Train Random Forest Classifier on Success vs. Ignore</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-differences-in-age,-income,-membership-for-factors-success-vs.-ignore\" data-toc-modified-id=\"Test-differences-in-age,-income,-membership-for-factors-success-vs.-ignore-6.4.1\">Test differences in age, income, membership for factors success vs. ignore</a></span></li><li><span><a href=\"#ANOVA-on-income-~-gender-+-offer-response\" data-toc-modified-id=\"ANOVA-on-income-~-gender-+-offer-response-6.4.2\">ANOVA on income ~ gender + offer response</a></span></li><li><span><a href=\"#Welch-ANOVA-on-income-~-offer-response\" data-toc-modified-id=\"Welch-ANOVA-on-income-~-offer-response-6.4.3\">Welch ANOVA on income ~ offer response</a></span></li></ul></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-6.5\">Random Forest Classifier</a></span></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-6.6\">Random Forest Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-the-decision-tree\" data-toc-modified-id=\"Visualize-the-decision-tree-6.6.1\">Visualize the decision tree</a></span></li><li><span><a href=\"#MANOVA\" data-toc-modified-id=\"MANOVA-6.6.2\">MANOVA</a></span></li></ul></li><li><span><a href=\"#Predict-BoGo-success-by-difficulty-based-on-gender,-age,-income-and-membership\" data-toc-modified-id=\"Predict-BoGo-success-by-difficulty-based-on-gender,-age,-income-and-membership-6.7\">Predict BoGo success by difficulty based on gender, age, income and membership</a></span></li><li><span><a href=\"#Functions-used-for-preprocessing-and-data-analysis\" data-toc-modified-id=\"Functions-used-for-preprocessing-and-data-analysis-6.8\">Functions used for preprocessing and data analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#create_summed_offer_response_cols(data,-inplace=True)\" data-toc-modified-id=\"create_summed_offer_response_cols(data,-inplace=True)-6.8.1\">create_summed_offer_response_cols(data, inplace=True)</a></span></li><li><span><a href=\"#add_offer_response_to_profile\" data-toc-modified-id=\"add_offer_response_to_profile-6.8.2\">add_offer_response_to_profile</a></span></li><li><span><a href=\"#check_indep_subjectgroups(df,-offer_str)\" data-toc-modified-id=\"check_indep_subjectgroups(df,-offer_str)-6.8.3\">check_indep_subjectgroups(df, offer_str)</a></span></li><li><span><a href=\"#calculate_anova(df,-offer,-dv,-iv,-welch=False)\" data-toc-modified-id=\"calculate_anova(df,-offer,-dv,-iv,-welch=False)-6.8.4\">calculate_anova(df, offer, dv, iv, welch=False)</a></span></li><li><span><a href=\"#get_customer_groups\" data-toc-modified-id=\"get_customer_groups-6.8.5\">get_customer_groups</a></span></li></ul></li></ul></li><li><span><a href=\"#Helper-functions\" data-toc-modified-id=\"Helper-functions-7\">Helper functions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This data set contains simulated data that mimics customer behavior on the Starbucks rewards mobile app. Once every few days, Starbucks sends out an offer to users of the mobile app. An offer can be merely an advertisement for a drink or an actual offer such as a discount or BOGO (buy one get one free). Some users might not receive any offer during certain weeks. \n",
    "\n",
    "Not all users receive the same offer, and that is the challenge to solve with this data set.\n",
    "\n",
    "Your task is to combine transaction, demographic and offer data to determine which demographic groups respond best to which offer type. This data set is a simplified version of the real Starbucks app because the underlying simulator only has one product whereas Starbucks actually sells dozens of products.\n",
    "\n",
    "Every offer has a validity period before the offer expires. As an example, a BOGO offer might be valid for only 5 days. You'll see in the data set that informational offers have a validity period even though these ads are merely providing information about a product; for example, if an informational offer has 7 days of validity, you can assume the customer is feeling the influence of the offer for 7 days after receiving the advertisement.\n",
    "\n",
    "You'll be given transactional data showing user purchases made on the app including the timestamp of purchase and the amount of money spent on a purchase. This transactional data also has a record for each offer that a user receives as well as a record for when a user actually views the offer. There are also records for when a user completes an offer. \n",
    "\n",
    "Keep in mind as well that someone using the app might make a purchase through the app without having received an offer or seen an offer.\n",
    "\n",
    "### Example\n",
    "\n",
    "To give an example, a user could receive a discount offer buy 10 dollars get 2 off on Monday. The offer is valid for 10 days from receipt. If the customer accumulates at least 10 dollars in purchases during the validity period, the customer completes the offer.\n",
    "\n",
    "However, there are a few things to watch out for in this data set. Customers do not opt into the offers that they receive; in other words, a user can receive an offer, never actually view the offer, and still complete the offer. For example, a user might receive the \"buy 10 dollars get 2 dollars off offer\", but the user never opens the offer during the 10 day validity period. The customer spends 15 dollars during those ten days. There will be an offer completion record in the data set; however, the customer was not influenced by the offer because the customer never viewed the offer.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "This makes data cleaning especially important and tricky.\n",
    "\n",
    "You'll also want to take into account that some demographic groups will make purchases even if they don't receive an offer. From a business perspective, if a customer is going to make a 10 dollar purchase without an offer anyway, you wouldn't want to send a buy 10 dollars get 2 dollars off offer. You'll want to try to assess what a certain demographic group will buy when not receiving any offers.\n",
    "\n",
    "### Final Advice\n",
    "\n",
    "Because this is a capstone project, you are free to analyze the data any way you see fit. For example, you could build a machine learning model that predicts how much someone will spend based on demographics and offer type. Or you could build a model that predicts whether or not someone will respond to an offer. Or, you don't need to build a machine learning model at all. You could develop a set of heuristics that determine what offer you should send to each customer (i.e., 75 percent of women customers who were 35 years old responded to offer A vs 40 percent from the same demographic to offer B, so send offer A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets\n",
    "\n",
    "The data is contained in three files:\n",
    "\n",
    "* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n",
    "* profile.json - demographic data for each customer\n",
    "* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n",
    "\n",
    "Here is the schema and explanation of each variable in the files:\n",
    "\n",
    "**portfolio.json**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings)\n",
    "\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "**transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours since start of test. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECTIVE\n",
    "Combine transaction, demographic and offer data to determine which demographic groups respond best to which offer type. This data set is a simplified version of the real Starbucks app because the underlying simulator only has one product whereas Starbucks actually sells dozens of products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.3.8, the latest is 0.3.9.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import pdb\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# statistical packages\n",
    "import pingouin\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "#% matplotlib inline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Portfolio data description**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(portfolio.shape)\n",
    "portfolio.sort_values(by=['offer_type', 'duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the offering structure, check difficulty (amount spent to get offer) vs. reward size for the different types of offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.groupby('offer_type')[['reward', 'difficulty']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that: \n",
    "* informational offers have placeholder values for reward and difficulty\n",
    "* difficulty to get a discount is higher on average than difficulty for a bogo. \n",
    "* At the same time, rewards for a discount are generally lower than for a bogo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile data description\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "profile.became_member_on = pd.to_datetime(profile.became_member_on, format='%Y%m%d')\n",
    "profile = profile.loc[profile.age < 100, :]\n",
    "\n",
    "# bin ages by steps of ten\n",
    "profile['age_group'] = np.ones((profile.shape[0], 1))*np.nan\n",
    "profile.loc[(profile.age <= 30), 'age_group'] = '< 30'\n",
    "profile.loc[(profile.age > 30) & (profile.age <= 50), 'age_group'] = '30 - 50'\n",
    "profile.loc[(profile.age > 50), 'age_group'] = '> 50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values\n",
    "\n",
    "* Roughly 13% of data have no information on gender or income\n",
    "* If a customer didn't provide income, s/he also didn't provide gender information\n",
    "\n",
    "These customers could be grouped in a separate group, as they likely don't want to be shown that we know something about them. They are perhaps data privacy sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.isnull().sum() / profile.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if people who didn't specify income also didn't specify gender info\n",
    "perc_wo_info = np.sum(profile.gender.isnull() & profile.income.isnull()) \\\n",
    "                      / sum(profile.income.isnull()) *100\n",
    "\n",
    "print(\"{} % of customers who didn't specify\".format(perc_wo_info),\n",
    "      \"income didn't specify gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for general bias in profile data\n",
    "\n",
    "There are some general tendencies in the customer group that we need to keep in mind for further analyses:\n",
    "\n",
    "* Female customers are slightly older on average than male or other customers\n",
    "* Female customers have on average the higher income than male or other customer (which could be due to the age difference)\n",
    "* Income significantly differs between young (<30), middle-aged (30-50) and older customers (>50)\n",
    "\n",
    "Based on these insights, it makes sense to add an age_group column to the profile dataframe.\n",
    "Note that analyses were done on a copy of the original dataframe. Thus, profile still contains NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out NaNs\n",
    "profile = profile.dropna(axis=0, how='any', inplace=True, subset=['income', 'age'])\n",
    "profile.groupby('gender')[['income', 'age']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')  # set to talk before exporting for larger font\n",
    "sns.set_palette('pastel')\n",
    "# YlGnBu_r\n",
    "g = sns.jointplot(x = df.age,\n",
    "                  y = df.income,\n",
    "                  hue = df.gender,\n",
    "                  dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing differences in income and age between gender groups\n",
    "\n",
    "From the above plot, we can derive the hypothesis that income varies with age group in the following form:\n",
    "- There seem to be large increases of income for every step from *18 to 30*, *30 to 50* and *above 50* years of age\n",
    "- There are more male customers in the lower income range compared to female customers. For high-income customers, differences are not as pronounced.\n",
    "- Age distribution in the sample is unequal with males being slightly younger\n",
    "\n",
    "We will test the above-mentioned hypotheses below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.anova(dv='income', between=['age', 'gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing differences in income by gender (main effect of gender)\n",
    "\n",
    "**Hypothesis**\n",
    "\n",
    "There are more male customers in the lower income range compared to female customers. For high-income customers, differences are not as pronounced.\n",
    "\n",
    "**Summary**\n",
    "* Female Starbucks customers have on average a higher income $(\\mu = 71,306\\ \\$, \\sigma = 22,338\\ \\$)$ compared to male $(\\mu = 61,194\\ \\$, \\sigma = 20,069\\ \\$)$ and other customers $(\\mu = 63,287\\ \\$, \\sigma = 18,938\\ \\$)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pairwise_tukey(dv='income', between='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing differences in income by age (main effect of age)\n",
    "\n",
    "**Hypothesis**\n",
    "\n",
    "There seem to be large increases of income for every step from *18 to 30*, *30 to 50* and *above 50* years of age\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Planned comparisons showed that:\n",
    "* Customers younger than 30 years of age earn less $(\\mu = 50,996\\ \\$)$ on average than middle-aged customers $(\\mu = 59,673\\ \\$)$ btw. 30 and 50 yrs, $t()=14.68, p=0.001$\n",
    "* Middle aged customers earn less $(\\mu = 59,673\\ \\$)$ on average than older customers over 50 yrs of age $(\\mu=70,598\\ \\$)$ with $t()=-28.17, p=0.001$\n",
    "* Young customers earn less than customers over 50 yrs of age $t()=-36.37, p=0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin ages by steps of ten\n",
    "df['age_group'] = np.ones((df.shape[0], 1))*np.nan\n",
    "df.loc[(df.age <= 30), 'age_group'] = '< 30'\n",
    "df.loc[(df.age > 30) & (df.age <= 50), 'age_group'] = '30 - 50'\n",
    "df.loc[(df.age > 50), 'age_group'] = '> 50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pairwise_tukey(dv='income', between='age_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing differences in age by gender\n",
    "\n",
    "**Hypothesis**\n",
    "\n",
    "Age distribution in the sample is unequal with males being slightly younger\n",
    "\n",
    "**Summary**\n",
    "\n",
    "There is a main effect of gender on age $F(2,14822) = 177.61, p < 0.01$ with the following pattern:\n",
    "* Female customers are older ($\\mu=58$, $\\sigma=17$) on average than male customers ($\\mu=52$, $\\sigma=17$), with $t(13434.5) = 18.92, p<0.01$\n",
    "* Female customers are older than LGBQ customers ($\\mu=54.4$, $\\sigma=16$) with $t(227) = 2.76, p<0.05$\n",
    "* There is no significant difference between male customers and LGBQ $t(223) = -2, p>0.05$\n",
    "* Sample size difference are very large for female $n=6129$ vs. other $n=212$ gender type, but non-parametric post-hoc tests result in the same pattern as parametric ones for the comparison of female vs. other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.anova(dv='age', between='gender', detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pairwise_ttests(dv='age', between='gender', padjust='bonf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=df, x='gender', y='age', \n",
    "              dodge=True, markers=['o', 's'],\n",
    "              capsize=.1, errwidth=1, \n",
    "              ci = 90,\n",
    "              estimator=np.mean);\n",
    "plt.xlabel('Gender [M]ale, [F]emale, [O]ther');\n",
    "plt.ylabel('Age in Years');\n",
    "plt.title('Mean age by gender');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete dataframe and variables generated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio data preprocessing\n",
    "\n",
    "* Map offer IDs to the ones we defined in the portfolio dataframe\n",
    "* Convert contact channel (list of strings) to separate columns with dummy coding\n",
    "* Add a column listing the number of contact channels used for the respective offer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "portfolio.sort_values(by=['offer_type', 'duration'], inplace=True)\n",
    "\n",
    "unique_offer_ids = portfolio.id.values\n",
    "offer_id = np.arange(1, len(unique_offer_ids)+1, 1)\n",
    "offer_id_mapping_dict = dict(zip(unique_offer_ids, offer_id))\n",
    "\n",
    "# do the mapping for portfolio df\n",
    "portfolio['offer_id'] = portfolio.id.map(offer_id_mapping_dict)\n",
    "portfolio.drop(columns='id', inplace=True)\n",
    "portfolio.sort_values(by='offer_id', inplace=True)\n",
    "\n",
    "# convert contact channels to dummies column\n",
    "portfolio = pd.concat(\n",
    "    [portfolio, \n",
    "    pd.get_dummies(portfolio.channels.apply(pd.Series).stack()).sum(level=0)],\n",
    "    axis=1)\n",
    "\n",
    "o_id_to_channel = zip(portfolio.offer_id.values, \n",
    "    np.sum(portfolio.loc[:,['email', 'mobile', 'social', 'web']], axis = 1))\n",
    "portfolio['n_channels'] = portfolio.offer_id.map(dict(o_id_to_channel))\n",
    "portfolio.drop(columns='channels', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript data preprocessing\n",
    "\n",
    "**Structure of transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours since start of test. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record\n",
    "\n",
    "**Preprocessing**\n",
    "* Map emails hashes to customer IDs as defined in the profile dataframe\n",
    "* Unpack value column and generate a separate column for offer_id and amount spent\n",
    "* add number of contact channels as column (information taken from portfolio DF)\n",
    "* add column that codes events (offer completed / received / ...) in numeric form\n",
    "* add offer duration as column\n",
    "* Convert membership to duration in days from start of membership until current date / time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)\n",
    "\n",
    "unique_p_ids = np.unique(transcript.person)\n",
    "p_id = np.arange(1, len(unique_p_ids)+1, 1)\n",
    "customer_id_mapping_dict = dict(zip(unique_p_ids, p_id))\n",
    "\n",
    "transcript['cust_id'] = transcript.person.map(customer_id_mapping_dict)\n",
    "\n",
    "del p_id, unique_p_ids\n",
    "\n",
    "# Generate a new column for offer ID by unpacking the values column. \n",
    "# Value can be either an offer_id with corresponding hash, or a transaction\n",
    "# amount\n",
    "\n",
    "transcript['offer_id'] = transcript['value'].apply(\n",
    "    lambda x: x.get('offer id'))\n",
    "transcript['offer_id2'] = transcript['value'].apply(\n",
    "    lambda x: x.get('offer_id'))\n",
    "transcript['amount'] = transcript['value'].apply(lambda x: x.get('amount'))\n",
    "\n",
    "transcript.loc[transcript.offer_id.isna(), 'offer_id'] = transcript.offer_id2\n",
    "transcript.drop(columns='offer_id2', inplace=True)\n",
    "\n",
    "transcript['n_channels'] = transcript.offer_id.map(dict(o_id_to_channel))\n",
    "\n",
    "transcript.head()\n",
    "\n",
    "# -------------------------------------------------------------------------- #\n",
    "# Perform some exploration on the data to better understand its structure\n",
    "# -------------------------------------------------------------------------- #\n",
    "out = transcript.offer_id.count() / transcript.shape[0]\n",
    "print('{}% of data contain an offer ID'.format(out),\n",
    "      'and are of an offer event type')\n",
    "\n",
    "out = transcript.amount.count() / transcript.shape[0]\n",
    "print('{}% of data contain an amount'.format(out),\n",
    "        'and are of a transaction event type')\n",
    "\n",
    "out = transcript.loc[(transcript.offer_id.isna() & \\\n",
    "                      transcript.amount.isna()),:].count()\n",
    "\n",
    "if out.any(): \n",
    "    print('There are still missing values in either amount or offer_id')\n",
    "else: print('There are no unaccounted-for rows')\n",
    "\n",
    "del out\n",
    "\n",
    "# do the mapping for transcript df. Transactions are mapped to offer_id=NaN\n",
    "transcript['offer_id'] = transcript.offer_id.map(offer_id_mapping_dict)\n",
    "transcript.drop(columns=['person', 'value'], inplace=True)\n",
    "\n",
    "# -------------------------------------------------------------------------- #\n",
    "# generate numeric column for customer events\n",
    "# -------------------------------------------------------------------------- #\n",
    "mapping_dict = {'offer completed':3, 'offer received':1, \n",
    "                'offer viewed':2, 'transaction':-1}\n",
    "transcript['event_num'] = transcript.event.map(mapping_dict)\n",
    "\n",
    "# -------------------------------------------------------------------------- #\n",
    "# add offer duration\n",
    "# -------------------------------------------------------------------------- #\n",
    "mapping_dict = dict(zip(portfolio.offer_id, portfolio.duration))\n",
    "transcript['o_duration'] = transcript.offer_id.map(mapping_dict)\n",
    "\n",
    "# -------------------------------------------------------------------------- #\n",
    "# create a time index in the transcript dataframe\n",
    "# -------------------------------------------------------------------------- #\n",
    "transcript.loc[:, 'time'] = pd.to_timedelta(transcript.time, unit='h', )\n",
    "transcript.set_index('time', inplace=True)\n",
    "transcript.loc[:, 'o_duration'] = pd.to_timedelta(transcript.o_duration, \n",
    "                                                  unit='h', )\n",
    "\n",
    "transcript.loc[~transcript.offer_id.isna(), 'offer_id'] = \\\n",
    "    transcript.loc[~transcript.offer_id.isna(), 'offer_id'].astype(int)\n",
    "\n",
    "del mapping_dict, offer_id, unique_offer_ids, offer_id_mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile data preprocessing\n",
    "* Map offer id to both the portfolio and the profile dataset using the dictionaries we built above based on the transcript dataset.\n",
    "* Convert date of first membership to duration of membership in days and add corresponding column\n",
    "* Add information per customer, how that customer responded to each offer s/he received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "profile.became_member_on = pd.to_datetime(profile.became_member_on, \n",
    "                                          format='%Y%m%d')\n",
    "\n",
    "# bin ages by steps of ten\n",
    "profile['age_group'] = np.ones((profile.shape[0], 1))*np.nan\n",
    "profile.loc[(profile.age <= 30), 'age_group'] = '< 30'\n",
    "profile.loc[(profile.age > 30) & (profile.age <= 50), 'age_group'] = '30 - 50'\n",
    "profile.loc[(profile.age > 50), 'age_group'] = '> 50'\n",
    "profile = profile.loc[profile.age < 100, :]\n",
    "\n",
    "profile['cust_id'] = profile.id.map(customer_id_mapping_dict)\n",
    "profile.drop(columns='id', inplace=True)\n",
    "profile.head()\n",
    "\n",
    "profile['gender_numeric'] = profile.gender.map({'F':1, 'M':2, \n",
    "                                                'O':3, None:None})\n",
    "\n",
    "profile['member_since'] = profile.became_member_on.dt.year.copy()\n",
    "\n",
    "# Add a column indicating time of membership in days from start of membership\n",
    "# compared to current date\n",
    "profile['member_for_days'] = profile.became_member_on.dt.to_pydatetime()\n",
    "today = datetime.date.today()\n",
    "profile['member_for_days'] = profile['member_for_days'].apply(\n",
    "    lambda t: today - t.date())\n",
    "profile['member_for_days'] = profile['member_for_days'].values.astype(\n",
    "    'timedelta64[D]')\n",
    "# Convert days to integer\n",
    "profile.member_for_days = profile.member_for_days.dt.days\n",
    "\n",
    "profile, success, missed, ignored, control = \\\n",
    "    add_offer_response_to_profile(portfolio, profile, transcript)\n",
    "\n",
    "# Check for duplicates\n",
    "print('There are {} duplicate rows in the profile dataframe'.format(\n",
    "    np.sum(profile.duplicated(subset=None))), ' based on all columns')\n",
    "\n",
    "print('There are {} duplicate rows in the profile dataframe'.format(\n",
    "    np.sum(profile.duplicated(subset='cust_id'))), ' based on cust_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------------------------------------------------------- #\n",
    "# # get customer groups for every offer_id, except the informational offers\n",
    "# # -------------------------------------------------------------------------- #\n",
    "# success = dict()\n",
    "# missed = dict()\n",
    "# ignored = dict()\n",
    "# control = dict()\n",
    "\n",
    "# for offer_id in np.unique(transcript.offer_id.values):\n",
    "#     v_not_c, c_not_v, neither_v_nor_c, conversion, view_after_completion = \\\n",
    "#         get_customer_groups(transcript, offer_id)\n",
    "#     success.update({offer_id : conversion})\n",
    "#     missed.update({offer_id : c_not_v.union(view_after_completion)})\n",
    "#     ignored.update({offer_id: v_not_c})\n",
    "#     control.update({offer_id: neither_v_nor_c})\n",
    "    \n",
    "# res = pd.DataFrame(columns=['success_rate', 'ignore_rate', 'missed_rate',\n",
    "#                                 'control_rate'],\n",
    "#                        index=np.unique(transcript.offer_id.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "## Exploratory analysis: offering logic\n",
    "\n",
    "To better understand the structure of the data and the setup of the 'experiment', the following section aims to answer the questions:\n",
    "* When are offers sent (regularly or not)\n",
    "* To how many customers are they sent?\n",
    "* Can we build subgroups based on the offer type that was sent?\n",
    "\n",
    "**Conclusion**\n",
    "* offers are sent every 7 days\n",
    "* there are 17,000 customers in the dataset\n",
    "* customers sets receiving an offer at each timepoint are neither equal nor disjoint\n",
    "* customer sets receiving an offer are similar sized at each timepoint (around 12,700 per point in time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cust = len(set(transcript.cust_id))\n",
    "print('There are {} customers in the dataset'.format(num_cust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cust_sets = dict.fromkeys(set(transcript.offer_id))\n",
    "\n",
    "for offer_id in np.arange(1, 10, 1):\n",
    "    df = transcript.loc[transcript.offer_id == offer_id, :]\n",
    "    print('Offer {} was sent at days: {}'.format(offer_id, \n",
    "          np.unique(df.loc[df.event == 'offer received', :].index.days))\n",
    "         )\n",
    "    cust_sets[offer_id] = set(df.loc[df.event == 'offer received', 'cust_id'])\n",
    "    print('{} customers for offer {}'.format(len(cust_sets[offer_id]), \n",
    "                                             offer_id))\n",
    "del df, offer_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a difference in effectiveness of individual offers, independent of subject?\n",
    "\n",
    "For the analyses to follow, four distinct customer groups were defined\n",
    "- **success**: customers who received, viewed and completed the offer\n",
    "- **miss**: customers who \n",
    "    - received the offer, completed it but did not view it\n",
    "    - received the offer, completed it but viewed it *after* completion\n",
    "- **ignore**: customers who received and viewed the offer but did not complete it\n",
    "- **control**: customers who did not complete the offer and did not see it\n",
    "\n",
    "Note that the same customer can belong to different groups for different offer id's. S/He can only belong to a single group for the same offer_id. Customer groups for the same offer_id are mutually exclusive and jointly exhaustive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if variables success, missed, ignored and control are not existing, please \n",
    "# make sure to have called all preprocessing functions for the profile\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------- #\n",
    "# transfer results into pandas DataFrame, calculate the percentage of total\n",
    "# customers for the different categories (success, missed, ignored, controls)\n",
    "# and merge with the offer information contained in the portfolio dictionary\n",
    "# -------------------------------------------------------------------------- #\n",
    "for offer_id in np.arange(1,9,1):\n",
    "    all_usr = len(np.unique(transcript.loc[transcript.offer_id == offer_id, \n",
    "                                         'cust_id'].values)) / 100\n",
    "    idx = portfolio.offer_id.values == offer_id\n",
    "    portfolio.loc[idx, 'success_rate'] = len(success.get(offer_id)) / all_usr\n",
    "    portfolio.loc[idx, 'ignore_rate'] = len(ignored.get(offer_id)) / all_usr\n",
    "    portfolio.loc[idx, 'missed_rate'] = len(missed.get(offer_id)) / all_usr\n",
    "    portfolio.loc[idx, 'control_rate'] = len(control.get(offer_id)) / all_usr\n",
    "    \n",
    "# Check success rates per customer and offer_id\n",
    "portfolio.sort_values(by='success_rate', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if success, miss, ignore and control rates differ significantly for offer type, difficulty (high vs. low) or reward (high vs. low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = portfolio.offer_type.isin(['bogo', 'discount'])\n",
    "contrasts = ['success_rate', 'ignore_rate', 'missed_rate', 'control_rate']\n",
    "t_stat = pd.DataFrame()\n",
    "\n",
    "for contrast in contrasts:\n",
    "    t_stat = t_stat.append(portfolio.loc[idx, :].pairwise_ttests(\n",
    "        dv = contrast, between='offer_type'))\n",
    "t_stat['comparison'] = contrasts\n",
    "t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio['bin_diff'] = portfolio.difficulty.values >= 10\n",
    "t_stat = pd.DataFrame()\n",
    "\n",
    "for contrast in contrasts:\n",
    "    t_stat = t_stat.append(portfolio.loc[idx, :].pairwise_ttests(\n",
    "        dv = contrast, between='bin_diff'))\n",
    "t_stat['comparison'] = contrasts\n",
    "\n",
    "t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio['bin_reward'] = portfolio.reward.values >= 10\n",
    "t_stat = pd.DataFrame()\n",
    "\n",
    "for contrast in contrasts:\n",
    "    t_stat = t_stat.append(portfolio.loc[idx, :].pairwise_ttests(\n",
    "        dv = contrast, between='bin_reward'))\n",
    "t_stat['comparison'] = contrasts\n",
    "\n",
    "t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio['many_channels'] = portfolio.n_channels.values > 3\n",
    "t_stat = pd.DataFrame()\n",
    "\n",
    "for contrast in contrasts:\n",
    "    t_stat = t_stat.append(portfolio.loc[idx, :].pairwise_ttests(\n",
    "        dv = contrast, between='n_channels'))\n",
    "t_stat['comparison'] = contrasts\n",
    "\n",
    "t_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the sets of subjects we defined are mutually exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_ids = list(success.keys())\n",
    "offer_ids.pop(-1)\n",
    "offer_ids.pop(-1)\n",
    "offer_ids2 = [x + 1 for x in offer_ids]\n",
    "for i in offer_ids:\n",
    "    for j in offer_ids2:\n",
    "        print('{:.0f} % customer overlap for completion on offers {} and {}'.format(\n",
    "                len(set(success[i]).intersection(set(success[j]))) /  \\\n",
    "                (len(set(success[i]).union(set(success[j]))) * 100), \n",
    "            i, j)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unique subjects per set: {} succ, {} miss, {} ign, {} ctrl'.format(\n",
    "    len(succ), len(miss), len(ign), len(ctrl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we improve results by weighting scores with attractiveness of the offer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weighted customer responses across offer_ids\n",
    "\n",
    "Add binary columns for each customer response (success, miss, ignore,..) that contain either a weighted or an unweighted sum of responses by offer attractivness for each customer\n",
    "\n",
    "1) a weighting score is calculated for each offer type taking into consideration the reward and the sum of inverse duration (which is a form of difficulty) and difficulty.\n",
    "\n",
    "2) add columns to the profile dataframe that contain the (weighted) sum of responses across offers for each customer. One column per response type is generated (success, miss, control, ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio['attractiveness'] = (portfolio.reward) / ((11 - portfolio.duration) + portfolio.difficulty)\n",
    "portfolio.sort_values(by='attractiveness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rather than just summing across event types, calculate a weighted sum\n",
    "# by weiging success or miss with attractiveness as calculated above\n",
    "bogo_cols = ['bogo_1', 'bogo_2', 'bogo_3', 'bogo_4']\n",
    "discount_cols = ['discount_5', 'discount_6', 'discount_7', 'discount_8']\n",
    "\n",
    "df = profile.loc[:, ['income', 'age_group', 'age', 'member_for_days', 'gender',\n",
    "                     *bogo_cols, *discount_cols]]\n",
    "\n",
    "# create a dictionary containing the offer_id column names (e.g. 'bogo_1') as\n",
    "# keys and the attractiveness rating as value\n",
    "offers = portfolio.offer_type.values + '_' + \\\n",
    "         portfolio.offer_id.values.astype(str)\n",
    "\n",
    "attr_map = dict(zip(offers, portfolio.attractiveness))\n",
    "attr_map.pop('informational_9')\n",
    "attr_map.pop('informational_10')\n",
    "\n",
    "weighted = False\n",
    "# Create the sum of all [successes, misses, ignores] weighted by \n",
    "# attractiveness of the offer\n",
    "for offer_id in attr_map.keys(): # goes through offer_ids\n",
    "    s_idx = (df.loc[:, offer_id] == 'succ').astype(int)\n",
    "    i_idx = (df.loc[:, offer_id] == 'ign').astype(int)\n",
    "    m_idx = (df.loc[:, offer_id] == 'miss').astype(int)\n",
    "    c_idx = (df.loc[:, offer_id] == 'ctrl').astype(int)\n",
    "    n_idx = (df.loc[:, offer_id] == 'nr').astype(int)\n",
    "    vals = dict(zip(['s', 'i', 'm', 'c'], [s_idx, i_idx, m_idx, c_idx]))\n",
    "\n",
    "    for cust_response, idx in vals.items():\n",
    "        if cust_response in df.columns:\n",
    "            if weighted: \n",
    "                df[cust_response] += idx * attr_map[offer_id]\n",
    "            else:\n",
    "                df[cust_response] += idx\n",
    "        else:\n",
    "            if weighted: \n",
    "                df[cust_response] = idx * attr_map[offer_id]\n",
    "            else: df[cust_response] = idx\n",
    "                \n",
    "        # also add columns 's1', 's2',... per offer_id\n",
    "        new_col = cust_response + offer_id[-1]\n",
    "        if weighted:\n",
    "            df[new_col] = idx * attr_map[offer_id]\n",
    "        else:\n",
    "            df[new_col] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(df[['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8']], axis=1).values - df.s.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Forest Classifier on Success vs. Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 0, how='any')\n",
    "\n",
    "# map offer_id (int) to a string in the form 'type_id' e.g. 'bogo_1'\n",
    "offer_map = dict(zip(portfolio.offer_id, \n",
    "    (portfolio.offer_type + '_' + portfolio.offer_id.values.astype(str))))\n",
    "\n",
    "# generate a plot title for each comparison in the dependent variable\n",
    "dv_names = ['Success {} vs. Ignored for offer: {}'.format(\n",
    "    x, offer_map[x]) for x in range(1, 9)]\n",
    "\n",
    "# set the independent variables / predictors\n",
    "iv = ['income', 'member_for_days', 'age']  \n",
    "\n",
    "sns.color_palette(\"rocket\", as_cmap=True)\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "# --------------------------------------------------------------------------#\n",
    "# Add all relevant comparisons to the dataframe\n",
    "# --------------------------------------------------------------------------#\n",
    "\n",
    "# generate a list of all comparisons to test (i.e. success vs. ignored for \n",
    "# each offer type)\n",
    "dv = ['s{}-i{}'.format(x, x) for x in range(1, 9)]\n",
    "\n",
    "for comparison in dv:\n",
    "    df[comparison] = df.loc[:, comparison[:2]] - df.loc[:, comparison[-2:]]\n",
    "\n",
    "# --------------------------------------------------------------------------#\n",
    "# Train the classifier\n",
    "# --------------------------------------------------------------------------#\n",
    "plot_results=False\n",
    "\n",
    "for dv_idx in range(0, len(dv)):\n",
    "    \n",
    "    X = df.loc[:, iv].values\n",
    "    Y = df.loc[:, dv[dv_idx]].values\n",
    "    print('X shape {}, Y shape {}'.format(X.shape, Y.shape))\n",
    "    X = X[Y != 0, :]\n",
    "    Y = Y[Y != 0]\n",
    "    \n",
    "    # generate training and test data for the model\n",
    "    X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    RF = RandomForestClassifier(n_estimators=1000, max_depth=10, \n",
    "                                criterion='entropy', \n",
    "                                random_state=0).fit(X_tr, Y_tr)\n",
    "    RF.predict(X_test)\n",
    "    score = round(RF.score(X_test, Y_test), 4)\n",
    "    scores.loc[dv_idx, 'acc'] = score\n",
    "    scores.loc[dv_idx, 'cond'] = dv[dv_idx]\n",
    "    \n",
    "    print('Prediction accuracy test set: {}'.format(score))\n",
    "\n",
    "    if plot_results:\n",
    "        # add .sort_values(ascending=False) if you want features by importance\n",
    "        feature_imp = pd.Series(RF.feature_importances_, index = iv)\n",
    "\n",
    "        plt.figure()\n",
    "        g = sns.barplot(x = feature_imp, y=feature_imp.index, \n",
    "                        alpha=0.7, palette='rocket')\n",
    "\n",
    "        # Add labels to your graph\n",
    "        plt.xlabel('Feature Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title(dv_names[dv_idx] + \\\n",
    "                  '\\n Accuracy {:0g}%'.format(round(score, 2)*100))\n",
    "        plt.show()\n",
    "        if weighted:\n",
    "            save_to = 'Figures/RF/weighted'\n",
    "        else:\n",
    "            save_to = 'Figures/RF/unweighted'\n",
    "        g.get_figure().savefig(save_to + 'Succ_vs_Ign_{}.pdf'.format(dv[dv_idx]))\n",
    "\n",
    "if ~weighted: \n",
    "    scores_uw = scores.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ~weighted: scores_uw = scores.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.set_index('cond');\n",
    "scores_uw.set_index('cond');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = pd.merge(left =scores, right=scores_uw, on='cond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test differences in age, income, membership for factors success vs. ignore\n",
    "\n",
    "Generate a pairplot (scatterplot + KDEs) comparing success vs. ignore and success vs. control for the factors age, income and membership duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = ['s1-i1', 's3-i3', 's2-i2', 's5-i5', 's8-i8', 's1-i1', 's8-i8', 's7-i7']\n",
    "# comparisons = ['s1-c1', 's3-c3', 's2-c2', 's5-c5', 's8-c8', 's1-c1', 's8-c8', 's7-c7']\n",
    "\n",
    "for cond in ['_w_zero', '_wo_zero']:\n",
    "    for comparison in comparisons:\n",
    "        df[comparison] = df.loc[:, comparison[:2]] - df.loc[:, comparison[-2:]]\n",
    "        if cond == '_wo_zero':\n",
    "            df_p = df.loc[df[comparison] != 0, :]\n",
    "        else:\n",
    "            df_p = df.copy()\n",
    "            \n",
    "        fig = sns.pairplot(df_p[['age', 'income', 'member_since', comparison]], \n",
    "                     hue=comparison)\n",
    "        fig.savefig(comparison + cond + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = ['s1-i1', 's2-i2', 's3-i3', 's4-i4', 's5-i5', 's6-i6', 's7-i7', 's8-i8']\n",
    "# comparisons = ['s1-m1', 's2-m2', 's3-m3', 's4-m4', 's5-m5', 's6-m6', 's7-m7', 's8-m8']\n",
    "df = df.loc[df.age < 100,:]\n",
    "\n",
    "for comparison in comparisons:\n",
    "        df.loc[:, comparison] = df.loc[:, comparison[:2]] - \\\n",
    "                                df.loc[:, comparison[-2:]]\n",
    "        df_p = df.loc[df[comparison] != 0, :]\n",
    "        fig = sns.jointplot(x=df_p.age, y=df_p.member_since, hue=df_p[comparison],\n",
    "                kind=\"kde\", marginal_kws={\"color\":\"r\", \"alpha\":.2}, \n",
    "                shade=True, thresh=0.05, alpha=.5)\n",
    "        fig.savefig('Jointplot_age_member_' + comparison + cond + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df.loc[df.loc[:, 's5-i5'] != 0, :].copy()\n",
    "df_p.loc[:, 'member_dur'] = 2020 - np.array(df_p.loc[:, 'member_since'].values)\n",
    "\n",
    "df_p.groupby('s5-i5')['member_dur'].hist(alpha=.5)\n",
    "plt.xlabel('Membership (years)');\n",
    "plt.ylabel('# offers converted vs. ignored');\n",
    "plt.title('Discount Offer, Nr. 5');\n",
    "plt.legend(['ignored', 'success'])\n",
    "plt.savefig('Membership_x_OfferResponse_Discount5.pdf')\n",
    "\n",
    "plt.figure()\n",
    "df_p = df.loc[df.loc[:, 's8-i8'] != 0, :].copy()\n",
    "df_p.loc[:, 'member_dur'] = 2020 - np.array(df_p.loc[:, 'member_since'].values)\n",
    "\n",
    "df_p.groupby('s8-i8')['member_dur'].hist(alpha=.5)\n",
    "plt.xlabel('Membership (years)');\n",
    "plt.ylabel('# offers converted vs. ignored');\n",
    "plt.title('Discount Offer, Nr. 8');\n",
    "plt.legend(['ignored', 'success'])\n",
    "plt.savefig('Membership_x_OfferResponse_Discount8.pdf')\n",
    "\n",
    "plt.figure()\n",
    "df_p = df.loc[df.loc[:, 's5-m5'] != 0, :].copy()\n",
    "df_p.loc[:, 'member_dur'] = 2020 - np.array(df_p.loc[:, 'member_since'].values)\n",
    "df_p.groupby('s5-m5')['member_dur'].hist(alpha=.5)\n",
    "plt.xlabel('Membership (years)');\n",
    "plt.ylabel('# offers converted vs. missed');\n",
    "plt.title('Discount Offer, Nr. 5');\n",
    "plt.legend(['missed', 'success'])\n",
    "plt.savefig('Membership_x_OfferResponse_Miss_Discount5.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA on income ~ gender + offer response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pingouin.homoscedasticity(df, dv='member_for_days', group='bogo_2', method='bartlett')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------#\n",
    "# Calculate a one-way independent samples ANOVA for all offer types\n",
    "# --------------------------------------------------------------------------#\n",
    "\n",
    "df = profile.copy()\n",
    "\n",
    "# drop \"Others\" gender category, otherwise group sizes are very unequal\n",
    "df = df.loc[df.gender.isin(['M', 'F']), :]\n",
    "\n",
    "# offers to test (response to the offer are independent variables)\n",
    "# responses considered are only success, ignore and control\n",
    "offers=['discount_5','discount_6','discount_7','discount_8', \n",
    "        'bogo_1', 'bogo_2', 'bogo_3', 'bogo_4']\n",
    "\n",
    "post_hoc = {}\n",
    "ph_df = pd.DataFrame()\n",
    "aov_res = pd.DataFrame()\n",
    "\n",
    "to_predict = 'member_for_days'\n",
    "\n",
    "for offer in offers:\n",
    "    aov, ph = calculate_anova(df, offer, to_predict, ['gender']) \n",
    "    post_hoc.update(ph)\n",
    "    \n",
    "    # for the main factor \"offer\", append results to dataframe object\n",
    "    ph[offer]['factor'] = np.repeat(offer, ph[offer].shape[0])\n",
    "    ph_df = ph_df.append(ph[offer])\n",
    "    \n",
    "    # for the main factor \"offer\", append results to dataframe object\n",
    "    aov['factor'] = np.repeat(offer, aov.shape[0])\n",
    "    aov_res = aov_res.append(aov)\n",
    "\n",
    "aov_res.set_index(['factor', 'Source'], drop=False, inplace=True)\n",
    "    \n",
    "# formatting for Styler object to render a readable table\n",
    "format_dict = {'p-unc':'{:,.4f}', 'DF':'{:,.0f}', 'ddof1':'{:,.0g}',\n",
    "               'ddof2':'{:,.0g}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov_res.style.format(format_dict, \n",
    "    subset = pd.IndexSlice[:, ['p-unc']]).applymap(\n",
    "    lambda x: \"color: green\" \\\n",
    "    if x < 0.01/8 else \"color: red\", \n",
    "    subset = pd.IndexSlice[:, ['p-unc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = 'income'\n",
    "make_anova_plot(df, dv, aov_res, ph_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welch ANOVA on income ~ offer response\n",
    "\n",
    "Perform Welch's ANOVA as this is more robust towards violations of non-homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------#\n",
    "# Calculate a one-way independent samples Welch ANOVA for all offer types\n",
    "# --------------------------------------------------------------------------#\n",
    "\n",
    "df = profile.copy()\n",
    "\n",
    "# drop \"Others\" gender category, otherwise group sizes are very unequal\n",
    "df = df.loc[df.gender.isin(['M', 'F']), :]\n",
    "\n",
    "# offers to test (response to the offer are independent variables)\n",
    "# responses considered are only success, ignore and control\n",
    "offers=['discount_5','discount_6','discount_7','discount_8', \n",
    "        'bogo_1', 'bogo_2', 'bogo_3', 'bogo_4']\n",
    "\n",
    "post_hoc = {}\n",
    "ph_df = pd.DataFrame()\n",
    "aov_res = pd.DataFrame()\n",
    "\n",
    "to_predict = 'income'  # 'member_for_days'\n",
    "\n",
    "for offer in offers:\n",
    "    aov, ph = calculate_anova(df, offer, to_predict, ['gender'], True) \n",
    "    post_hoc.update(ph)\n",
    "\n",
    "    # for the main factor \"offer\", append results to dataframe object\n",
    "    ph[offer]['factor'] = np.repeat(offer, ph[offer].shape[0])\n",
    "    ph_df = ph_df.append(ph[offer])\n",
    "\n",
    "    # for the main factor \"offer\", append results to dataframe object\n",
    "    aov['factor'] = np.repeat(offer, aov.shape[0])\n",
    "    aov_res = aov_res.append(aov)\n",
    "\n",
    "# formatting for Styler object to render a readable table\n",
    "format_dict = {'p-unc':'{:,.6f}', 'DF':'{:,.0f}', 'ddof1':'{:4.0f}',\n",
    "               'ddof2':'{:4,.0f}', 'np2':'{:,.2f}'}\n",
    "aov_res.set_index('factor', inplace=True)\n",
    "\n",
    "aov_res.sort_index().style.format(format_dict, \n",
    "    subset = pd.IndexSlice[:, ['p-unc']]).applymap(\n",
    "    lambda x: \"color: green\" \\\n",
    "    if x < 0.01/8 else \"color: red\", \n",
    "    subset = pd.IndexSlice[:, ['p-unc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for offer_id in offers:\n",
    "    data = df.loc[df[offer_id].isin(['succ', 'ign', 'miss'])]\n",
    "    plt.figure()\n",
    "    sns.kdeplot(x='income', data=data, hue=offer_id)\n",
    "    plt.xlabel('Income in USD');\n",
    "    plt.title(offer_id)\n",
    "    plt.savefig('Figures/Welch/Welch_ANOVA_income_x_offer_{}.pdf'.format(offer_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Predict conversion on easy and difficult BoGo offers by age, gender, income and membership duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profile.copy()\n",
    "\n",
    "# map offer_id (int) to a string in the form 'type_id' e.g. 'bogo_1'\n",
    "offers = portfolio.offer_type.values + '_' + portfolio.offer_id.values.astype(str)\n",
    "\n",
    "sns.color_palette(\"rocket\", as_cmap=True)\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "# set the independent variables / predictors\n",
    "iv = ['income', 'member_for_days']\n",
    "\n",
    "# define which levels of the dependent variable (=offer response) to consider\n",
    "target_names_excl = ['ctrl', 'miss', 'nr']\n",
    "target_names = set(df[offers[0]].values).difference(target_names_excl)\n",
    "\n",
    "# --------------------------------------------------------------------------#\n",
    "# Train the classifier\n",
    "# --------------------------------------------------------------------------#\n",
    "plot_results=False\n",
    "i = 0\n",
    "\n",
    "classifiers = dict.fromkeys(offers)\n",
    "\n",
    "# drop \"Others\" gender category, otherwise group sizes are very unequal\n",
    "df = df.loc[df.gender.isin(['M', 'F']), :]\n",
    "df.gender = (df.gender == 'M').astype(int)\n",
    "\n",
    "for offer in offers:\n",
    "\n",
    "    nr_idx = df.loc[:, offer].isin(target_names_excl)\n",
    "    X = df.loc[~nr_idx, iv].values\n",
    "    Y = df.loc[~nr_idx, offer].values\n",
    "    # print('X shape {}, Y shape {}'.format(X.shape, Y.shape))\n",
    "    \n",
    "    # generate training and test data for the model\n",
    "    X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    RF = RandomForestClassifier(n_estimators=2, max_depth=3, \n",
    "                                criterion='gini', \n",
    "                                random_state=0).fit(X_tr, Y_tr)\n",
    "    RF.predict(X_test)\n",
    "    classifiers[offer] = RF\n",
    "    \n",
    "    score = round(RF.score(X_test, Y_test), 4)\n",
    "    scores.loc[i, 'acc'] = score\n",
    "    scores.loc[i, 'cond'] = offer\n",
    "    \n",
    "    for f_idx in range(0, len(iv)):\n",
    "        scores.loc[i, iv[f_idx]] = RF.feature_importances_[f_idx]\n",
    "    print('Prediction accuracy test set: {}'.format(score))\n",
    "    i += 1\n",
    "    \n",
    "    if plot_results:\n",
    "        # add .sort_values(ascending=False) if you want features by importance\n",
    "        feature_imp = pd.Series(RF.feature_importances_, index = iv)\n",
    "\n",
    "        plt.figure()\n",
    "        g = sns.barplot(x = feature_imp, y=feature_imp.index, \n",
    "                        alpha=0.7, palette='rocket')\n",
    "\n",
    "        # Add labels to your graph\n",
    "        plt.xlabel('Feature Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title(dv_names[dv_idx] + \\\n",
    "                  '\\n Accuracy {:0g}%'.format(round(score, 2)*100))\n",
    "        plt.show()\n",
    "        if weighted:\n",
    "            save_to = 'Figures/RF/weighted'\n",
    "        else:\n",
    "            save_to = 'Figures/RF/unweighted'\n",
    "        g.get_figure().savefig(save_to + 'Succ_vs_Ign_{}.pdf'.format(dv[dv_idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(by='cond')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for offer in offers:\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (4,4), dpi=800)\n",
    "    tree.plot_tree(classifiers[offer].estimators_[0],\n",
    "                   feature_names = iv, \n",
    "                   class_names = list(target_names),\n",
    "                   filled = True, impurity=True,\n",
    "                   proportion=True, rounded = True, precision=2);\n",
    "    fig.savefig('Figures/RF/rf_tree_{}.png'.format(offer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = profile.copy()\n",
    "df = df.loc[df.gender.isin(['F', 'M'])]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'income + member_for_days ~ bogo_4'\n",
    "maov = MANOVA.from_formula(formula, data = df)\n",
    "print(maov.mv_test())\n",
    "\n",
    "print('Results on success')\n",
    "print('--------------------------------------------------------')\n",
    "reg = ols('income ~ bogo_4 + bogo_2 + bogo_3', data = df).fit()\n",
    "aov = sm.stats.anova_lm(reg, type=2)\n",
    "print(aov)\n",
    "\n",
    "print('\\nResults on ignored')\n",
    "print('--------------------------------------------------------')\n",
    "reg = ols('member_for_days ~ bogo_4 + bogo_2 + bogo_3', data = df).fit()\n",
    "aov = sm.stats.anova_lm(reg, type=2)\n",
    "print(aov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profile.dropna(axis = 0, how='any')\n",
    "\n",
    "# feature_names = ['succ_bogo', 'ign_bogo', 'ctrl_bogo', 'miss_bogo']\n",
    "feature_names = ['succ_discount', 'ign_discount', \n",
    "                 'ctrl_discount', 'miss_discount']\n",
    "# feature_names = ['income', 'member_since']\n",
    "to_predict = ['income']\n",
    "\n",
    "for col in to_predict:\n",
    "    X = df[feature_names]\n",
    "    Y = df.loc[:, col]\n",
    "\n",
    "    # generate training and test data for the model\n",
    "    X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    RF = RandomForestClassifier(n_estimators=1000, max_depth=10, \n",
    "                                criterion='entropy', \n",
    "                                random_state=0).fit(X_tr, Y_tr)\n",
    "    RF.predict(X_test)\n",
    "    score = round(RF.score(X_test, Y_test), 4)\n",
    "    print('Prediction accuracy on test set: {}'.format(score))\n",
    "\n",
    "    feature_imp = pd.Series(RF.feature_importances_, \n",
    "                        index = feature_names).sort_values(ascending=False)\n",
    "    sns.barplot(x = feature_imp, y=feature_imp.index)\n",
    "    # Add labels to your graph\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Classification results for {}. Score {}'.format(col, score))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict BoGo success by difficulty based on gender, age, income and membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profile.dropna(axis = 0, how='any')\n",
    "\n",
    "# predict easy and difficult buy_one_get_one_free offer\n",
    "df['bogo_difficult'] = \\\n",
    "        np.sum(profile.loc[:, ['bogo_1', 'bogo_3']].isin(['succ']).astype(int), axis=1)\n",
    "\n",
    "df['bogo_easy'] = \\\n",
    "        np.sum(profile.loc[:, ['bogo_2', 'bogo_4']].isin(['succ']).astype(int), axis=1)\n",
    "\n",
    "feature_names = ['age', 'gender_numeric', 'income', 'member_since']\n",
    "to_predict = ['bogo_difficult', 'bogo_easy']\n",
    "\n",
    "for col in to_predict:\n",
    "    X = df[feature_names]\n",
    "    Y = df.loc[:, col]\n",
    "\n",
    "    # generate training and test data for the model\n",
    "    X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    RF = RandomForestClassifier(n_estimators=1000, max_depth=10, \n",
    "                                criterion='entropy', \n",
    "                                random_state=0).fit(X_tr, Y_tr)\n",
    "    RF.predict(X_test)\n",
    "    score = round(RF.score(X_test, Y_test), 4)\n",
    "    print('Prediction accuracy on test set: {}'.format(score))\n",
    "\n",
    "    feature_imp = pd.Series(RF.feature_importances_, \n",
    "                        index = feature_names).sort_values(ascending=False)\n",
    "    sns.barplot(x = feature_imp, y=feature_imp.index)\n",
    "    # Add labels to your graph\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Classification results for {}. Score {}'.format(col, score))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used for preprocessing and data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_summed_offer_response_cols(data, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summed_offer_response_cols(data, inplace=True):\n",
    "    \"\"\" Creates columns containing summed column responses across offer IDs \n",
    "        for each offer category (bogo vs. discount).\n",
    "        Separate columns are generated per response type (ignore, miss, \n",
    "        control, success)\n",
    "        \n",
    "        Args:\n",
    "        ----------------------------------------------\n",
    "        data (df) : profile dataframe containing subject and offer response\n",
    "                    information\n",
    "                       \n",
    "        inplace (bool) : whether to return a separate df or modify in place\n",
    "                         default is True\n",
    "                         \n",
    "        Returns: None or profile DataFrame object, depending on inplace value\n",
    "    \n",
    "    \"\"\"\n",
    "    if ~inplace: profile = data.copy()\n",
    "    \n",
    "    for val in ['succ', 'ign', 'miss', 'ctrl']:\n",
    "        colname = '{}_bogo'.format(val)\n",
    "        cols = ['bogo_1', 'bogo_2', 'bogo_3', 'bogo_4']\n",
    "        profile[colname] = \\\n",
    "            np.sum(profile.loc[:, cols].isin([val]).astype(int), axis=1)\n",
    "\n",
    "    for val in ['succ', 'ign', 'miss', 'ctrl']:\n",
    "        colname = '{}_discount'.format(val)\n",
    "        cols = ['discount_5', 'discount_6', 'discount_7', 'discount_8']\n",
    "        profile[colname] = \\\n",
    "            np.sum(profile.loc[:, cols].isin([val]).astype(int), axis=1)\n",
    "    \n",
    "    if ~inplace: return profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add_offer_response_to_profile\n",
    "\n",
    "Adding success, miss, ignored and control information to the profile dataframe\n",
    "Append one column for each offer_id that contains the following number code\n",
    "* **succ** : offer received, viewed and completed (success)\n",
    "* **ign** : offer received, viewed but not completed (ignored)\n",
    "* **miss** : offer received, completed but either not viewed or viewed after completion (missed)\n",
    "* **ctrl** : offer received, not viewed, not completed (control)\n",
    "* **nr** : customers who did not receive the offer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_offer_response_to_profile(portfolio, profile, transcript):\n",
    "\n",
    "# append one column for each o_id that contains the following number code\n",
    "# succ : offer success\n",
    "# ign : offer ignored\n",
    "# miss : offer missed\n",
    "# ctrl : control (received, not viewed, not converted)\n",
    "# nr : customers who did not receive the offer\n",
    "\n",
    "# -------------------------------------------------------------------------- #\n",
    "# get customer groups for every o_id, except the informational offers\n",
    "# -------------------------------------------------------------------------- #\n",
    "    success = dict()\n",
    "    missed = dict()\n",
    "    ignored = dict()\n",
    "    control = dict()\n",
    "\n",
    "    for o_id in np.unique(transcript.offer_id.values):\n",
    "        print(o_id)\n",
    "        v_not_c, c_not_v, neither_v_nor_c, conversion, view_after_completion = \\\n",
    "            get_customer_groups(transcript=transcript, offer_id=o_id)\n",
    "        success.update({o_id : conversion})\n",
    "        missed.update({o_id : c_not_v.union(view_after_completion)})\n",
    "        ignored.update({o_id: v_not_c})\n",
    "        control.update({o_id: neither_v_nor_c})\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # get the subject group and mark the respective result for that customer\n",
    "        # and o_id combination\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        idx = portfolio.offer_id == o_id\n",
    "        col_name = '{}_{}'.format(portfolio.loc[idx, 'offer_type'].values[0], \n",
    "                                  portfolio.loc[idx, 'offer_id'].values[0])\n",
    "\n",
    "        profile.loc[profile.cust_id.isin(success.get(o_id)), o_id] = 'succ'\n",
    "        profile.loc[profile.cust_id.isin(ignored.get(o_id)), o_id] = 'ign'\n",
    "        profile.loc[profile.cust_id.isin(missed.get(o_id)), o_id] = 'miss'\n",
    "        profile.loc[profile.cust_id.isin(control.get(o_id)), o_id] = 'ctrl'\n",
    "        profile.rename(columns = {o_id:col_name}, inplace=True)\n",
    "        profile.loc[profile[col_name].isna(), col_name] = 'nr'\n",
    "\n",
    "    return profile, success, missed, ignored, control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_anova_plot(df, dv, aov_res, ph_df):\n",
    "    \"\"\" Plots results of a two_way ANOVA as generated by \n",
    "        calculate_anova\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    map_dict = {'nr':'not received', 'miss':'missed', 'succ':'converted',\n",
    "                'ctrl':'control', 'ign':'ignored'}\n",
    "    f = lambda x: map_dict[x]\n",
    "    \n",
    "    # get level 1 factors from anova results table\n",
    "    level_1 = aov_res.index.get_level_values(0).to_numpy()\n",
    "    \n",
    "    for factor1 in set(aov_res.factor):\n",
    "        \n",
    "        # get second factor from second row in ANOVA table\n",
    "        factor2 = aov_res.loc[factor1].index[0]\n",
    "        print('Independent Variables: {} and {}'.format(factor1, factor2))\n",
    "        \n",
    "        # only proceed if there is a significant effect of offer response\n",
    "        pval = aov_res.loc[factor1, factor1]['p-unc']\n",
    "        if pval >= 0.001/len(set(aov_res.factor)): break\n",
    "\n",
    "        plt.figure()\n",
    "        \n",
    "        # filter out customers who missed the offer or did not receive it\n",
    "        data =  df.loc[df.loc[:, factor1].isin(['succ', 'ign' , 'ctrl']), :]\n",
    "        \n",
    "        g = sns.pointplot(data = data, x = factor1, y = dv, hue = factor2,\n",
    "                      dodge = True, markers = ['o', 's'],\n",
    "                      capsize = .1, errwidth = 1, \n",
    "                      ci = 90,  # bootstrap and plot the 90% conf. interval\n",
    "                      estimator = np.median)\n",
    "        # pdb.set_trace()\n",
    "        plt.xlabel('Response to offer {}'.format(factor1))\n",
    "        ax = plt.gca()\n",
    "        labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "        new_labels = [f(x) for x in labels]\n",
    "        \n",
    "        plt.xticks(ticks = range(0,len(new_labels)), labels = new_labels)   \n",
    "        \n",
    "        plt.ylabel(dv)\n",
    "        plt.title('{} ~ {} + {}'.format(dv, factor1, factor2))\n",
    "        plt.savefig('Figures/anova_{}_{}.pdf'.format(dv, factor1), \n",
    "                    bbox_inches='tight', \n",
    "                    pad_inches=0.3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check_indep_subjectgroups(df, offer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_indep_subjectgroups(df, offer_str):\n",
    "    \"\"\" Verifies if customer groups in dataframe are mutually exclusive.\n",
    "        Throws assertion error if groups are not mutually exclusive.\n",
    "        \n",
    "        Args:\n",
    "        ---------------------------\n",
    "        df (df) : dataframe containing already the offer responses per\n",
    "                  subject (added by get_customer_groups())\n",
    "        \n",
    "        offer_str (str) : string describing the offer type column, which \n",
    "                          consists of the format {offer_type}_{offer_id},\n",
    "                          e.g. 'bogo_1'\n",
    "                          \n",
    "        Returns: None\n",
    "    \"\"\"\n",
    "    g = df.groupby(offer_str)\n",
    "    assert set(g.get_group('succ').cust_id.values).intersection(\n",
    "             set(g.get_group('ign').cust_id.values)).intersection(\n",
    "                set(g.get_group('ctrl').cust_id.values)) == set([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_anova(df, offer, dv, iv, welch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anova(df, offer, dv, iv, welch=False):\n",
    "    \"\"\" Calculates either a Welch or a standard one- or n-way ANOVA\n",
    "        and returns both ANOVA results as well as post-hoc tests using \n",
    "        Games-Howell correction\n",
    "        \n",
    "        Args:\n",
    "        ---------------------------\n",
    "        df (df) : dataframe containing already the offer responses per\n",
    "                  subject (added by get_customer_groups())\n",
    "                  \n",
    "        offer_str (str) : string describing the offer type column, which \n",
    "                          consists of the format {offer_type}_{offer_id},\n",
    "                          e.g. 'bogo_1'\n",
    "        \n",
    "        dv (str) : name of the dependent variable, must be column in df\n",
    "        \n",
    "        iv (list) : list of name or names of the independent variable(s)\n",
    "                    Must be column in df\n",
    "        \n",
    "        welch (bool) : default False. Whether to calculate standard (default)\n",
    "                       or Welch ANOVA\n",
    "                    \n",
    "        Returns:\n",
    "        ---------------------------\n",
    "        aov (df) : dataframe containing ANOVA output\n",
    "        \n",
    "        post_hoc (dict) : dictionary of dataframes that contains results\n",
    "                          of the post-hoc tests. Different factors can be\n",
    "                          accessed by key tuples. E.g. main effect of offer\n",
    "                          is accessed by post_hoc['bogo_1'], the main \n",
    "                          effect of gender by post_hoc[('bogo_1', 'gender')]\n",
    "                          to avoid overwriting gender values.\n",
    "    \n",
    "    \"\"\"\n",
    "    # as we do not use repeated_measures anova, ensure mutual exclusivity\n",
    "    check_indep_subjectgroups(df, offer)\n",
    "    \n",
    "    iv = iv + [offer]\n",
    "    \n",
    "    # get names of the independent variables to test\n",
    "    cols = [dv] + iv\n",
    "    \n",
    "    # filtered out missed responses and calculate anova\n",
    "    data =  df.loc[df.loc[:, offer].isin(['succ', 'ign' , 'ctrl']), cols]\n",
    "    \n",
    "    if welch:\n",
    "        aov = data.welch_anova(dv = dv, between = offer)\n",
    "    else:\n",
    "        aov = data.anova(dv = dv, between = iv)\n",
    "    \n",
    "    # generate result dictionary with tuples of (offer_type, factorname)\n",
    "    # example: ('bogo_1', 'gender'), ('bogo_1', 'income'),...\n",
    "    post_hoc = dict.fromkeys(zip(np.repeat(offer, len(iv)), iv))\n",
    "    \n",
    "    # add post hoc test results to dictionary\n",
    "    for factor in iv:\n",
    "        if offer == factor:\n",
    "            post_hoc[offer] = pingouin.pairwise_gameshowell(\n",
    "                data = data, dv = dv, between = factor)\n",
    "        else:\n",
    "            post_hoc[(offer, factor)] = pingouin.pairwise_gameshowell(\n",
    "                data = data, dv = dv, between = factor)\n",
    "\n",
    "    \n",
    "    return aov, post_hoc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_customer_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_groups(transcript, offer_id):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "        ---------------------------\n",
    "        transcript (df) : dataframe containing information on offer_id and \n",
    "                          customer events such as offer viewed, completed, \n",
    "                          received\n",
    "        \n",
    "        offer_id (int) : number of the offer to analyse. The number must be \n",
    "                         contained in the column 'offer_id' transcript\n",
    "       \n",
    "        Returns:\n",
    "        ---------------------------\n",
    "        v_not_c (set) : customers who viewed the offer but did not complete it\n",
    "        \n",
    "        c_not_v (set) : customers who completed the offer but didn't view it\n",
    "        \n",
    "        neither_v_nor_c (set) : customers who neither viewed nor completed\n",
    "        \n",
    "        conversion (set) : customers who viewed the offer before completion\n",
    "                           and completed it\n",
    "                           \n",
    "        view_after_completion (set) : customers who viewed the offer only \n",
    "                                      after having completed it\n",
    "    \"\"\"\n",
    "    offer_id_group = transcript.groupby('offer_id')\n",
    "    offer_id_group = offer_id_group.get_group(offer_id)\n",
    "    event_group = offer_id_group.groupby(by=['event'])\n",
    "\n",
    "    all_cust = set(offer_id_group.cust_id.values)\n",
    "    \n",
    "    # offer IDs 9 and 10 are informational offers which cannot be completed\n",
    "    if offer_id < 9:\n",
    "        completed = set(\n",
    "            event_group.get_group('offer completed').cust_id.values)\n",
    "    else:\n",
    "        completed = set([])\n",
    "    viewed = set(event_group.get_group('offer viewed').cust_id.values)\n",
    "\n",
    "    # customers who viewed the offer but did not complete it\n",
    "    v_not_c = viewed - completed\n",
    "\n",
    "    # customers who completed the offer but did not view it\n",
    "    c_not_v = completed - viewed\n",
    "\n",
    "    # customers who viewed the offer AND completed it (intersection)\n",
    "    v_and_c = viewed & completed \n",
    "\n",
    "    # customers who neither viewed nor completed the offer - control group\n",
    "    neither_v_nor_c = all_cust - completed - viewed\n",
    "\n",
    "    conversion = [] # conversion group, i.e. people who saw and used the offer\n",
    "    view_after_completion = [] # people who saw the offer but did not complete it\n",
    "\n",
    "    # for customers that viewed and received the offer, compare timestamps\n",
    "    for customer in v_and_c:\n",
    "        t_view = offer_id_group.loc[(offer_id_group.cust_id == customer) & \\\n",
    "                          (offer_id_group.event == 'offer viewed')].index \n",
    "        if offer_id < 9:\n",
    "            t_complete = offer_id_group.loc[ \\\n",
    "                (offer_id_group.cust_id == customer) & \\\n",
    "                (offer_id_group.event == 'offer completed')].index\n",
    "            if np.array([t < t_complete for t in t_view]).any():\n",
    "                conversion.append(customer)\n",
    "            else:\n",
    "                view_after_completion.append(customer)\n",
    "            \n",
    "    print('Number of subjects per category found for offer_id: {}'.format(offer_id))\n",
    "    print('Ignored: viewed but not converted: {}'.format(len(v_not_c)))\n",
    "    print('Missed: converted but not viewed: {}'.format(len(c_not_v)))\n",
    "    print('Control: neither viewed nor converted: {}'.format(len(neither_v_nor_c)))\n",
    "    print('Missed: viewed after completion: {}'.format(len(ignored)))\n",
    "    print('Success: viewed and converted: {}'.format(len(conversion)))\n",
    "\n",
    "    # Assert that customer groups defined are mutually exclusive\n",
    "    assert set(v_not_c).intersection(set(c_not_v), set(neither_v_nor_c),\n",
    "                                     set(ignored), set(conversion)) == set()\n",
    "    \n",
    "    return v_not_c, c_not_v, neither_v_nor_c, conversion, view_after_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_not_c, c_not_v, neither_v_nor_c, conversion, view_after_completion = \\\n",
    "    get_customer_groups(transcript, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_post_hoc_tests(data, labels, indices, alpha=0.05, \n",
    "                         bonferroni=True, parametric=True):\n",
    "    \"\"\" Performs independent-samples post-hoc tests a given columns in the\n",
    "        input dataframe. If parametric = True, ttest is used, otherwise the\n",
    "        Mann-Whitney U test is used.\n",
    "        \n",
    "       \n",
    "       Args:\n",
    "       ---------------------------\n",
    "        data (list) : data used to calculate the test statistic. \n",
    "                      List of lists due to potential inequal sample sizes\n",
    "        \n",
    "        labels (list) : list of strings detailing the comparison for each\n",
    "                        pair of groups\n",
    "        \n",
    "        indices (list) : list of indices used for calculating the test\n",
    "        \n",
    "        alpha (int) : alpha level used for the test\n",
    "        \n",
    "        bonferroni (boolean) : whether to use Bonferroni correction on alpha \n",
    "                               for the number of tests performed.\n",
    "       \n",
    "       Returns:\n",
    "       ---------------------------\n",
    "        df_print : Dataframe containing test results\n",
    "    \"\"\"\n",
    "    \n",
    "    # use Bonferroni correction by dividing alpha by the #post-hoc test\n",
    "    \n",
    "    if bonferroni:\n",
    "        alpha = (alpha / len(labels))\n",
    "        print('Alpha value after Bonferroni correction: ', alpha, '\\n')\n",
    "    \n",
    "    pairs = zip(indices, labels)\n",
    "    \n",
    "    df_print = pd.DataFrame(columns=['label', 'Pval', 'is_significant'])\n",
    "    \n",
    "    # calculate pairwise, non-parametric post-hoc tests\n",
    "    count = 0\n",
    "    for idx, label in pairs:\n",
    "        if parametric:\n",
    "            stat, pval = ttest_ind(data[idx[0]], data[idx[1]])\n",
    "            df_print = df_print.append(\n",
    "                pd.DataFrame({'label':label, 'Pval':pval, \n",
    "                              'T-stat':stat, 'is_significant':(pval<alpha)}, \n",
    "                              index = [count]))\n",
    "        else:\n",
    "            stat, pval = mannwhitneyu(data[idx[0]], data[idx[1]])\n",
    "            df_print = df_print.append(\n",
    "                pd.DataFrame({'label':label, 'Pval':pval, \n",
    "                              'U-stat':stat, 'is_significant':(pval<alpha)}, \n",
    "                              index = [count]))\n",
    "        count += 1;\n",
    "    return df_print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "603px",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
